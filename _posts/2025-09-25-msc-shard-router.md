---
layout: post
image: assets/images/system-design/sharding-route-capa.png
author: matheus
featured: false
published: true
categories: [ msc ]
title: Msc. Field Notes - Shard Router 
---

> Este artigo faz parte de uma organiza√ß√£o de um material bruto excedente da minha tese de mestrado. Tem o objetivo de compilar as referencias tecnicas e experimenta√ß√µes pr√°ticas. 


Este compilado foca no desenvolvimento e an√°lise de um **roteador baseado em dar suporte em celulas ou shards** de bu;lheads, componente inicial para implementa√ß√µes arquitetura celular. 

O projeto se baseia na aplica√ß√£o simples e conceitual de padr√µes de arquitetura de roteamento **celular** ou **bulkheads**, que implementa **roteamento determin√≠stico baseado em hashing consistente**. O roteador celular atua como um proxy reverso especializado que direciona requisi√ß√µes de clientes para c√©lulas (shards) espec√≠ficas, garantindo que requisi√ß√µes de um mesmo cliente sejam sempre processadas pela mesma c√©lula ou shard.

- **Hashing Consistente**: Para distribui√ß√£o uniforme e est√°vel de requisi√ß√µes
- **Roteamento Determin√≠stico**: Garantindo que clientes sejam sempre direcionados √† mesma c√©lula
- **Isolamento de Falhas**: Atrav√©s de bulkheads implementados a n√≠vel de roteamento
- **Observabilidade Granular**: Com m√©tricas espec√≠ficas por c√©lula e algoritmo de hash

<br>

## Fundamenta√ß√£o Te√≥rica

### Sharding e Particionamento Horizontal

O sharding, ou particionamento horizontal, √© uma t√©cnica consolidada para distribui√ß√£o de dados e processamento em sistemas distribu√≠dos (√ñzsu & Valduriez, 2020). Diferentemente do particionamento vertical, que divide dados por colunas ou atributos, o sharding divide o conjunto de dados em parti√ß√µes horizontais baseadas em crit√©rios espec√≠ficos, como ranges de valores ou fun√ß√µes de hash. Esse conceito √© diretamente associado a particionamento de dados fisicamente em bancos de dados, mas n√£o se limita a eles. Iremos seguir aqui pra frente como um crit√©rio de segmenta√ß√£o total de infraestrutura, cliente e demais recursos. 

<div class="mermaid">
graph TD
    subgraph "Particionamento Vertical"
        PV1[Tabela Original]
        PV2[Colunas A, B] 
        PV3[Colunas C, D]
        PV4[Colunas E, F]
        
        PV1 --> PV2
        PV1 --> PV3
        PV1 --> PV4
    end
</div>

<div class="mermaid">
graph TD    
    subgraph "Sharding"
        PH1[Dataset Completo]
        PH2[Shard A<br/>Users 1-1000]
        PH3[Shard B<br/>Users 1001-2000]
        PH4[Shard C<br/>Users 2001-3000]
        
        PH1 --> PH2
        PH1 --> PH3
        PH1 --> PH4
    end
</div>

<br>

## An√°lise de Blast Radius e Disponibilidade Sist√™mica

### Conceito de Blast Radius em Arquiteturas Distribu√≠das

O **blast radius** (raio de explos√£o) representa o escopo de impacto de uma falha em sistemas distribu√≠dos. Em arquiteturas celulares, o blast radius √© diretamente proporcional ao n√∫mero de c√©lulas distribu√≠das, oferecendo uma rela√ß√£o matem√°tica clara entre disponibilidade e granularidade de distribui√ß√£o.

A f√≥rmula fundamental para c√°lculo de disponibilidade em caso de falha de uma c√©lula √©:

```
Disponibilidade = ((N - F) / N) √ó 100%

Onde:
- N = N√∫mero total de c√©lulas/shards
- F = N√∫mero de c√©lulas falhando
```

<br>

### Impacto da Granularidade na Disponibilidade

A an√°lise quantitativa demonstra como o aumento do n√∫mero de c√©lulas reduz exponencialmente o blast radius:

**Rela√ß√£o Blast Radius vs. N√∫mero de C√©lulas**

| N√∫mero de C√©lulas | Falhas (1 c√©lula) | Disponibilidade | Blast Radius | Clientes Afetados |
|-------------------|-------------------|-----------------|--------------|-------------------|
| 3 | 1 | 66.7% | 33.3% | 1/3 da base |
| 5 | 1 | 80.0% | 20.0% | 1/5 da base |
| 10 | 1 | 90.0% | 10.0% | 1/10 da base |
| 25 | 1 | 96.0% | 4.0% | 1/25 da base |
| 50 | 1 | 98.0% | 2.0% | 1/50 da base |
| 100 | 1 | 99.0% | 1.0% | 1/100 da base |
| 1000 | 1 | 99.9% | 0.1% | 1/1000 da base |


<br>
<br>

### Trade-offs Operacionais

O aumento da granularidade celular apresenta trade-offs que devem ser considerados:

#### Benef√≠cios da Alta Granularidade

**Redu√ß√£o de Blast Radius**:
- 10 c√©lulas: Falha afeta 10% dos usu√°rios
- 100 c√©lulas: Falha afeta 1% dos usu√°rios
- 1000 c√©lulas: Falha afeta 0.1% dos usu√°rios

**Isolamento Melhorado**:
- Falhas ficam contidas em dom√≠nios menores
- Debugging e troubleshooting mais focado
- Rollbacks afetam menos usu√°rios

<br>

#### Custos da Alta Granularidade

**Complexidade Operacional vs. Granularidade Celular**

| Aspecto | Baixa Granularidade<br/>(3-10 C√©lulas) | M√©dia Granularidade<br/>(25-50 C√©lulas) | Alta Granularidade<br/>(100+ C√©lulas) |
|---------|----------------------------------------|------------------------------------------|---------------------------------------|
| **Complexidade Geral** | Moderada | Moderada | Alta |
| **Monitoramento** | Simples<br/>- Poucos endpoints<br/>- Dashboards b√°sicos | Estruturado<br/>- Alertas configurados<br/>- M√©tricas agregadas | Sofisticado<br/>- Observabilidade avan√ßada<br/>- APM necess√°rio |
| **Deployment** | Automatizado<br/>- CI/CD recomendado<br/>- Blue/Green deploy | Automatizado<br/>- CI/CD recomendado<br/>- Blue/Green deploy | CI/CD Avan√ßado<br/>- Canary releases<br/>- |
| **Recursos Computacionais** | Baixo<br/>- 3-10 inst√¢ncias<br/>- Overhead m√≠nimo | Moderado<br/>- 25-50 inst√¢ncias<br/>- Overhead controlado | Alto<br/>- 100+ inst√¢ncias<br/>- Overhead significativo |
| **Custo Operacional** | Baixo | M√©dio | Alto |

<br>
<br>

**Overhead Operacional Detalhado**:

- **Recursos Computacionais**: Aumento linear proporcional ao n√∫mero de c√©lulas
- **Monitoramento e Observabilidade**: Necessidade de ferramentas sofisticadas (Prometheus, Grafana, Jaeger)
- **Automa√ß√£o**: Obrigat√≥ria para granularidade alta, opcional para baixa granularidade
- **Equipe Especializada**: Requisitos crescentes de expertise em SRE e DevOps

<br>
<br>

### Modelo Matem√°tico de Disponibilidade

Para m√∫ltiplas falhas simult√¢neas, o modelo estende-se para:

```
Disponibilidade = ((N - F) / N) √ó 100%

Exemplos pr√°ticos:
- 100 c√©lulas, 2 falhas: ((100-2)/100) = 98% dispon√≠vel
- 100 c√©lulas, 5 falhas: ((100-5)/100) = 95% dispon√≠vel
- 1000 c√©lulas, 10 falhas: ((1000-10)/1000) = 99% dispon√≠vel
```

<br>

### Implementa√ß√£o na Prova de Conceito

A PoC desenvolvida permite configura√ß√£o din√¢mica do n√∫mero de c√©lulas atrav√©s de vari√°veis de ambiente:

```bash
# Configura√ß√£o para baixo blast radius (alta granularidade)
export SHARD_01_URL="http://cell-001:8080"
export SHARD_02_URL="http://cell-002:8080"
...
export SHARD_100_URL="http://cell-100:8080"

# Resultado: 1% blast radius por falha
```

O roteador automaticamente distribui a carga entre todas as c√©lulas configuradas, garantindo que a falha de qualquer c√©lula individual afete apenas 1/N da base de usu√°rios.

<br>

### Racional pr√°tico de blast radius

Com base na an√°lise de blast radius, recomenda-se:

1. **Startups/Pequenas Aplica√ß√µes**: 5-10 c√©lulas (blast radius: 10-20%)
2. **Aplica√ß√µes M√©dias**: 25-50 c√©lulas (blast radius: 2-4%)  
3. **Aplica√ß√µes Cr√≠ticas**: 100+ c√©lulas (blast radius: <1%)
4. **Sistemas de Alta Disponibilidade**: 1000+ c√©lulas (blast radius: <0.1%)


<br>

## Implementa√ß√£o e Aspectos T√©cnicos

A implementa√ß√£o da PoC utiliza sharding baseado em chaves de identifica√ß√£o de clientes, conforme evidenciado na estrutura de configura√ß√£o:

```go
type ShardRouterImpl struct {
    hashRing    interfaces.HashRing
    shardingKey string
}

func (sr *ShardRouterImpl) GetShardingKey(r *http.Request) string {
    return r.Header.Get(sr.shardingKey)
}
```

Esta abordagem garante que todas as requisi√ß√µes de um determinado cliente sejam consistentemente direcionadas √† mesma c√©lula, propriedade fundamental para manuten√ß√£o de estado e cache locality (DeCandia et al., 2007).

<br>

### Hashing Consistente

O hashing consistente, introduzido por Karger et al. (1997), resolve limita√ß√µes do hashing tradicional em ambientes distribu√≠dos din√¢micos. Enquanto o hashing simples requer redistribui√ß√£o global de chaves quando n√≥s s√£o adicionados ou removidos, o hashing consistente minimiza a movimenta√ß√£o de dados, redistribuindo apenas uma fra√ß√£o das chaves.

<div class="mermaid">
graph TB
    subgraph "Hashing Tradicional"
        HT1[3 Servidores]
        HT2[Key % 3]
        HT3[Server 0: 33%]
        HT4[Server 1: 33%] 
        HT5[Server 2: 33%]
        
        HT1 --> HT2
        HT2 --> HT3
        HT2 --> HT4
        HT2 --> HT5
        
        HT6[ +1 Servidor]
        HT7[Key % 4]
        HT8[75% das chaves<br/>redistribu√≠das]
        
        HT6 --> HT7
        HT7 --> HT8
    end

    style HT8 fill:#ffcdd2
</div>

<div class="mermaid">
graph TB

    subgraph "Hashing Consistente"
        HC1[Hash Ring]
        HC2[Virtual Replicas]
        HC3[Minimal Redistribution]
        
        HC1 --> HC2
        HC2 --> HC3
        
        HC4[ +1 Servidor]
        HC5[25%<br/>das chaves movidas]
        
        HC4 --> HC5
    end
    
    style HC5 fill:#c8e6c9
</div>

A PoC implementa m√∫ltiplos algoritmos de hash, permitindo an√°lise comparativa de desempenho e distribui√ß√£o:

```go
const (
    MD5     HashAlgorithm = "MD5"
    SHA1    HashAlgorithm = "SHA1" 
    SHA256  HashAlgorithm = "SHA256"
    SHA512  HashAlgorithm = "SHA512"
    MURMUR3 HashAlgorithm = "MURMUR3"
)
```

Estudos emp√≠ricos realizados com a implementa√ß√£o revelam varia√ß√µes significativas na qualidade da distribui√ß√£o entre algoritmos. O SHA1 apresentou a menor vari√¢ncia (121.67) e diferen√ßa entre melhor e pior shard (2.7%), enquanto algoritmos n√£o-criptogr√°ficos como FNV64 demonstraram distribui√ß√£o inadequada (vari√¢ncia de 156,116.33).

<div class="mermaid">
graph TB
    subgraph "Compara√ß√£o"
        CAH1[Input: client-id]
        
        subgraph "Algoritmos de Hashing"
            AC1[SHA1<br/>‚úÖ Melhor Distribui√ß√£o<br/>Desvio: 11.03]
            AC2[SHA256<br/>‚ö†Ô∏è Distribui√ß√£o Moderada<br/>Desvio: 64.60]
            AC3[SHA512<br/>‚úÖ Boa Distribui√ß√£o<br/>Desvio: 28.31]
            AC4[MD5<br/>‚ö†Ô∏è Distribui√ß√£o Aceit√°vel<br/>Desvio: 42.05]

            ANC1[MURMUR3<br/>‚ùå Distribui√ß√£o Irregular<br/>Desvio: 95.84]
            ANC2[FNV64<br/>‚ùå Distribui√ß√£o Inadequada<br/>Desvio: 395.12]

        end
        
        subgraph "Distribui√ß√£o nos Shards"
            DS1[Shard A: 32-34%]
            DS2[Shard B: 30-35%] 
            DS3[Shard C: 31-37%]
        end
        
        CAH1 --> AC1
        CAH1 --> AC2
        CAH1 --> AC3
        CAH1 --> AC4
        CAH1 --> ANC1
        CAH1 --> ANC2
        
        AC1 --> DS1
        AC1 --> DS2
        AC1 --> DS3
    end
    
    style AC1 fill:#c8e6c9
    style AC3 fill:#e8f5e8
    style AC2 fill:#fff3e0
    style AC4 fill:#fff3e0
    style ANC1 fill:#ffcdd2
    style ANC2 fill:#ffcdd2
</div>

<br>

### Bulkheads e Isolamento de Falhas

O padr√£o Bulkhead, inspirado na constru√ß√£o naval, prop√µe a compartimentaliza√ß√£o de sistemas para conter falhas (Nygard, 2018). Na arquitetura celular, cada c√©lula funciona como um bulkhead independente, onde falhas em uma c√©lula n√£o propagam para outras c√©lulas do sistema.

<div class="mermaid">
graph TB
    subgraph "Arquitetura sem Bulkheads"
        AB1[Load Balancer]
        AB2[Shared Resource Pool]
        AB3[Service A]
        AB4[Service B] 
        AB5[Service C]
        AB6[üí• Falha em cascata]
        
        AB1 --> AB2
        AB2 --> AB3
        AB2 --> AB4
        AB2 --> AB5
        AB3 -.->|falha propaga| AB4
        AB4 -.->|falha propaga| AB5
        AB5 --> AB6
    end

    
    style AB6 fill:#ffcdd2
</div>

<div class="mermaid">
graph TB    
    subgraph "Arquitetura com Bulkheads (Celular)"
        BC1[Shard Router]
        
        subgraph "C√©lula B"
            APPB[App B] --> DBB[Database B]
        end

        subgraph "C√©lula A - Falha Isolada"
            APPA[App A] --> DBA[Database A] --> FALHA[üí• Falha no Shard]
        end

        subgraph "C√©lula C"
            APPC[App C] --> DBC[Database C]
        end
        
        BC1 --> APPA
        BC1 --> APPB
        BC1 --> APPC
    end
    
    style FALHA fill:#ffcdd2
</div>

A implementa√ß√£o demonstra este isolamento atrav√©s da estrutura de proxy reverso:

```go
func (ph *ProxyHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    shardKey := ph.router.GetShardingKey(r)
    shardURL := ph.router.GetShardHost(shardKey)
    
    // Isolamento: falha em um shard n√£o afeta outros
    client := &http.Client{}
    resp, err := client.Do(proxyReq)
    if err != nil {
        http.Error(w, err.Error(), http.StatusBadGateway)
        return
    }
}
```

<br>

### Observabilidade e M√©tricas

A observabilidade √© crucial para opera√ß√£o de sistemas distribu√≠dos (Majors et al., 2022). A PoC implementa coleta de m√©tricas usando Prometheus, fornecendo visibilidade sobre:

- Distribui√ß√£o de requisi√ß√µes por shard
- Taxa de sucesso/falha por c√©lula  
- Lat√™ncia de processamento
- Utiliza√ß√£o de recursos

<div class="mermaid">
graph TB
    subgraph "Sistema Observ√°vel"
        SO1[Cellular Router]
        
        subgraph "C√©lulas"
            SC1[C√©lula A]
            SC2[C√©lula B]
            SC3[C√©lula C]
        end
        
        subgraph "Coleta de M√©tricas"
            CM1[Prometheus Metrics]
            CM2[Request Counter]
            CM3[Response Counter]
            CM4[Health Checks]
        end
        
        subgraph "Visualiza√ß√£o"
            SV1[Grafana Dashboard]
            SV2[üìä Distribui√ß√£o por Shard]
            SV3[üìà Taxa de Sucesso/Falha]
            SV4[‚è±Ô∏è Lat√™ncia por C√©lula]
            SV5[üéØ Detec√ß√£o de Hotspots]
        end
        
        SO1 --> SC1
        SO1 --> SC2
        SO1 --> SC3
        
        SC1 --> CM1
        SC2 --> CM1
        SC3 --> CM1
        
        CM1 --> CM2
        CM1 --> CM3
        CM1 --> CM4
        
        CM1 --> SV1
        SV1 --> SV2
        SV1 --> SV3
        SV1 --> SV4
        SV1 --> SV5
    end
    
    style CM1 fill:#e3f2fd
    style SV1 fill:#e8f5e8
    style SV5 fill:#fff3e0
</div>

<br>

```go
type PrometheusMetricsRecorder struct {
    requestsCounter prometheus.CounterVec
    responseCounter prometheus.CounterVec
}

func (pm *PrometheusMetricsRecorder) RecordRequest(shard string) {
    pm.requestsCounter.WithLabelValues(shard).Inc()
}
```

<br>

## Arquitetura da Solu√ß√£o

### Vis√£o Geral do Sistema

A arquitetura implementada na PoC segue o padr√£o de proxy reverso com roteamento baseado em hashing consistente. O sistema √© composto por tr√™s camadas principais:

1. **Camada de Roteamento**: Respons√°vel por receber requisi√ß√µes e determinar o shard de destino
2. **Camada de Hash Ring**: Implementa o algoritmo de hashing consistente 
3. **Camada de C√©lulas**: Conjunto de servi√ßos independentes (shards)

<div class="mermaid">
graph TB
    subgraph "Cliente"
        C1[Aplica√ß√£o Cliente]
        C2[Header: client-id]
    end
    
    subgraph "Camada de Roteamento"
        R1[HTTP Server :8080]
        R2[Proxy Handler]
        R3[Shard Router]
    end
    
    subgraph "Camada Hash Ring"
        H1[Consistent Hash Ring]
        H2[SHA-512 Algorithm]
        H3[Virtual Replicas]
    end
    
    subgraph "C√©lulas (Shards)"
        S1[C√©lula A<br/>Domain Shard]
        S2[C√©lula B<br/>Domain Shard] 
        S3[C√©lula C<br/>Domain Shard]
        SN[C√©lula N<br/>Domain Shard]
    end
    
    subgraph "Observabilidade"
        M1[Prometheus Metrics]
        M2[Health Checks]
    end
    
    C1 --> C2
    C2 --> R1
    R1 --> R2
    R2 --> R3
    R3 --> H1
    H1 --> H2
    H2 --> H3
    
    H3 --> S1
    H3 --> S2 
    H3 --> S3
    H3 --> SN
    
    R2 --> M1
    R1 --> M2
    
    style S1 fill:#e1f5fe
    style S2 fill:#e1f5fe
    style S3 fill:#e1f5fe
    style SN fill:#e1f5fe
</div>

<br>

### Fluxo de Processamento

O fluxo de processamento de uma requisi√ß√£o na arquitetura celular segue as seguintes etapas:

<div class="mermaid">
sequenceDiagram
    participant C as Cliente
    participant P as Proxy Router
    participant H as Hash Ring
    participant S as Shard (C√©lula)
    participant M as M√©tricas
    
    C->>P: HTTP Request + client-id header
    P->>P: Extrair sharding key
    P->>H: GetNode(client-id)
    H->>H: Calcular hash SHA-512
    H->>H: Localizar no ring
    H-->>P: URL do shard destino
    P->>M: Registrar requisi√ß√£o
    P->>S: Proxy request
    S-->>P: Response
    P->>M: Registrar resposta
    P-->>C: HTTP Response
</div>

<br>

### Algoritmo de Distribui√ß√£o

O algoritmo de hashing consistente implementado utiliza r√©plicas virtuais para melhorar a distribui√ß√£o uniforme das chaves:

<div class="mermaid">
graph LR
    subgraph "Hash Ring"
        direction TB
        R1[Replica Shard-1-0<br/>Hash: 0x1A2B]
        R2[Replica Shard-2-0<br/>Hash: 0x3C4D]  
        R3[Replica Shard-1-1<br/>Hash: 0x5E6F]
        R4[Replica Shard-3-0<br/>Hash: 0x7890]
        R5[Replica Shard-2-1<br/>Hash: 0x9ABC]
        R6[Replica Shard-3-1<br/>Hash: 0xDEF0]
    end
    
    K1[Key: user-123<br/>Hash: 0x4567] --> R3
    K2[Key: user-456<br/>Hash: 0x8901] --> R6
    K3[Key: user-789<br/>Hash: 0x2345] --> R2
    
    style R1 fill:#ffcdd2
    style R3 fill:#ffcdd2
    style R2 fill:#c8e6c9
    style R5 fill:#c8e6c9
    style R4 fill:#e1bee7
    style R6 fill:#e1bee7
</div>

<br>

## An√°lise de Desempenho dos Algoritmos de Hash

### Metodologia de Avalia√ß√£o

Para validar a efic√°cia dos diferentes algoritmos de hash na distribui√ß√£o uniforme de chaves, foram realizados experimentos com **1 milh√£o de chaves UUID v4** distribu√≠das entre 3 shards. As chaves UUID v4 foram escolhidas por sua natureza aleat√≥ria e representatividade em cen√°rios reais de produ√ß√£o. Os crit√©rios de avalia√ß√£o inclu√≠ram:

- **Uniformidade de distribui√ß√£o**: Medida pelo desvio padr√£o da distribui√ß√£o
- **Vari√¢ncia**: Indicador de dispers√£o dos valores  
- **Diferen√ßa m√°xima**: Dist√¢ncia entre o shard com maior e menor carga

<br>

### Resultados Experimentais

A Tabela 1 apresenta os resultados comparativos dos algoritmos testados:

**An√°lise Comparativa de Algoritmos de Hash**

| Algoritmo | Desvio Padr√£o | Vari√¢ncia | Melhor Shard (%) | Pior Shard (%) | Diferen√ßa |
|-----------|---------------|-----------|------------------|----------------|----------|
| SHA1 | 11.03 | 121.67 | 32.0% | 34.7% | 2.7% |
| SHA512 | 28.31 | 801.67 | 30.5% | 37.2% | 6.7% |  
| SHA256 | 64.60 | 4173.67 | 26.3% | 41.9% | 15.6% |
| MD5 | 42.05 | 1768.33 | 28.2% | 38.5% | 10.3% |
| MURMUR3 | 95.84 | 9185.33 | 23.1% | 48.2% | 25.1% |

<br> <br>

### Discuss√£o dos Resultados

Os resultados evidenciam que o **SHA1** apresenta a melhor distribui√ß√£o uniforme, com menor desvio padr√£o (11.03) e diferen√ßa entre shards (2.7%). Este comportamento contraria expectativas iniciais que favoreciam SHA-512 devido √† maior complexidade criptogr√°fica.

O **SHA-512**, embora apresente distribui√ß√£o aceit√°vel (desvio padr√£o: 28.31), demonstra performance inferior ao SHA1 em termos de uniformidade. Contudo, mant√©m caracter√≠sticas criptogr√°ficas superiores, relevantes para cen√°rios que exigem resist√™ncia a ataques de hash.

Algoritmos n√£o-criptogr√°ficos como **MURMUR3** apresentaram distribui√ß√£o menos uniforme que esperado, contradizendo literatura que sugere sua superioridade em aplica√ß√µes de hashing distribu√≠do (Appleby, 2008).

<br>

## Propriedades da Arquitetura Celular

### Determinismo de Roteamento

Uma propriedade fundamental da arquitetura celular √© o determinismo de roteamento. Requisi√ß√µes com a mesma chave de sharding s√£o consistentemente direcionadas √† mesma c√©lula, independentemente do momento da requisi√ß√£o ou estado do sistema.

```go
func (sr *ShardRouterImpl) GetShardHost(key string) string {
    node := sr.hashRing.GetNode(key)
    fmt.Printf("[%s] Mapping key %s to host: %s\n", 
               sr.hashRing.GetHashAlgorithm(), key, node)
    return node
}
```

Esta propriedade √© essencial para:
- Manuten√ß√£o de cache local por c√©lula
- Consist√™ncia de sess√£o de usu√°rio
- Otimiza√ß√£o de consultas relacionadas por cliente

### Escalabilidade Horizontal

A arquitetura permite adi√ß√£o din√¢mica de c√©lulas sem interrup√ß√£o do servi√ßo. O uso de hashing consistente garante redistribui√ß√£o m√≠nima de chaves (aproximadamente K/N chaves movidas, onde K √© o total de chaves e N o n√∫mero de n√≥s).

### Toler√¢ncia a Falhas

O isolamento entre c√©lulas proporciona conten√ß√£o de falhas. A indisponibilidade de uma c√©lula afeta apenas os clientes mapeados para aquela c√©lula espec√≠fica, mantendo o restante do sistema operacional.

### Observabilidade Granular

O roteamento determin√≠stico facilita observabilidade granular por c√©lula, permitindo:
- M√©tricas espec√≠ficas por dom√≠nio de clientes
- Detec√ß√£o de hotspots de tr√°fego  
- An√°lise de padr√µes de uso por segmento

<br>

## Implementa√ß√£o e Aspectos T√©cnicos

### Padr√µes de Projeto Aplicados

A implementa√ß√£o utiliza diversos padr√µes consolidados:

**Strategy Pattern**: Para algoritmos de hash intercambi√°veis
```go
type HashAlgorithm string
const (
    SHA512  HashAlgorithm = "SHA512"
    SHA256  HashAlgorithm = "SHA256"
    // ...
)
```

**Proxy Pattern**: Para roteamento transparente de requisi√ß√µes
```go
type ProxyHandler struct {
    router          interfaces.ShardRouter
    metricsRecorder interfaces.MetricsRecorder
}
```

**Factory Pattern**: Para cria√ß√£o de componentes configur√°veis
```go
func NewConsistentHashRing(numReplicas int) interfaces.HashRing {
    ring := &ConsistentHashRing{
        Nodes:       []Node{},
        NumReplicas: numReplicas,
    }
    ring.configureHashAlgorithm()
    return ring
}
```

<br>

## Limita√ß√µes e Trabalhos Futuros

### Limita√ß√µes Identificadas

1. **Rebalanceamento**: N√£o h√° implementa√ß√£o autom√°tica de rebalanceamento quando c√©lulas ficam sobrecarregadas
2. **Descoberta de Servi√ßos**: Configura√ß√£o est√°tica de shards limita elasticidade
3. **Consist√™ncia Cross-Cell**: Transa√ß√µes que envolvem m√∫ltiplas c√©lulas n√£o s√£o suportadas
4. **Circuit Breaker**: Aus√™ncia de prote√ß√£o contra cascata de falhas

### Extens√µes Propostas

1. **Auto-scaling Celular**: Algoritmos para adi√ß√£o/remo√ß√£o autom√°tica de c√©lulas baseado em m√©tricas de carga
2. **Service Mesh Integration**: Integra√ß√£o com Istio/Linkerd para descoberta de servi√ßos e pol√≠ticas de tr√°fego
3. **Distributed Tracing**: Implementa√ß√£o de rastreamento distribu√≠do para an√°lise de lat√™ncia cross-cell
4. **Consensus Protocols**: Integra√ß√£o com Raft/PBFT para coordena√ß√£o entre c√©lulas


<br>

## Refer√™ncias

Appleby, A. (2008). *MurmurHash3*. SMHasher. https://github.com/aappleby/smhasher

DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., ... & Vogels, W. (2007). Dynamo: Amazon's highly available key-value store. *ACM SIGOPS operating systems review*, 41(6), 205-220. https://doi.org/10.1145/1323293.1294281

Karger, D., Lehman, E., Leighton, T., Panigrahy, R., Levine, M., & Lewin, D. (1997). Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web. *Proceedings of the twenty-ninth annual ACM symposium on Theory of computing*, 654-663. https://doi.org/10.1145/258533.258660

Majors, C., Fong-Jones, L., & Miranda, G. (2022). *Observability engineering: Achieving production excellence*. O'Reilly Media.

Newman, S. (2015). *Building microservices: Designing fine-grained systems*. O'Reilly Media.

Nygard, M. T. (2018). *Release it!: Design and deploy production-ready software* (2nd ed.). Pragmatic Bookshelf.

√ñzsu, M. T., & Valduriez, P. (2020). *Principles of distributed database systems* (4th ed.). Springer. https://doi.org/10.1007/978-3-030-26253-2

Richardson, C. (2018). *Microservices patterns: With examples in Java*. Manning Publications.


{% include mermaid.html %}